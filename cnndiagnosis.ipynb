{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6799a957-b400-4254-ae49-b33bbfd5e3a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "# AS usual, a bit of setup\n",
    "# If you need other libraries, you should import the libraries.\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import os, sys\n",
    "import torch\n",
    "from torch import Tensor, nn, optim\n",
    "from torch.nn import functional as F\n",
    "\n",
    "import torchvision\n",
    "from torchvision import models\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.datasets as datasets\n",
    "from torch.utils.data import TensorDataset, DataLoader, random_split, Dataset\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import cv2\n",
    "import csv\n",
    "\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "from torchvision.transforms.functional import to_pil_image\n",
    "from torchvision.transforms.functional import pil_to_tensor\n",
    "import PIL\n",
    "from matplotlib import cm\n",
    "from PIL import Image\n",
    "\n",
    "import glob\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f592e0d8-f7ab-4fbb-810e-5da7d6015dab",
   "metadata": {},
   "outputs": [],
   "source": [
    "#######################################################\n",
    "#               Define Transforms\n",
    "#######################################################\n",
    "\n",
    "#Array for Augmentations,... (X5 Times of Train Dataset) \n",
    "\n",
    "#To Tensor and Normalize as of grayscale data. \n",
    "transform_train = []\n",
    "for num in range(5):\n",
    "    transform_train.append(transforms.Compose([\n",
    "        #transforms.RandomCrop(32, padding=4),#[1]\n",
    "        transforms.RandomRotation(degrees = num * 3 + 3 ),\n",
    "        transforms.ToTensor(),\n",
    "        #Normalize 데이터는 recipe를 참고하였음. https://github.com/Armour/pytorch-nn-practice/blob/master/utils/meanstd.py \n",
    "        transforms.Normalize((0.5),(0.5)) \n",
    "    ]))\n",
    "\n",
    "# Normalize test set same as training set without augmentation\n",
    "transform_test = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    #Normalize 데이터는 recipe를 참고하였음. https://github.com/Armour/pytorch-nn-practice/blob/master/utils/meanstd.py \n",
    "    transforms.Normalize((0.5),(0.5)) \n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f9c8fbd-dc42-43fb-82b2-d468331df7f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "####################################################\n",
    "#       Create Train, Valid and Test sets\n",
    "####################################################\n",
    "\n",
    "raw_images = [] #to store image in list\n",
    "raw_answer = [] # to store class values\n",
    "data = []\n",
    "path = \"/Users/seungjoobaek/Downloads/small_train_data_set\"\n",
    "img_size = 128\n",
    "\n",
    "with open('/Users/seungjoobaek/Downloads/small_train_data_set/train_data_1.csv') as list:\n",
    "    reader_obj = csv.reader(list)\n",
    "    for row in reader_obj:\n",
    "        try :\n",
    "            with open(os.path.join(path, row[2]), 'r') as f :\n",
    "                img_arr = cv2.imread(os.path.join(path, row[2]), cv2.IMREAD_GRAYSCALE)\n",
    "                resized_arr = cv2.resize(img_arr, (img_size,img_size))\n",
    "                raw_images.append(resized_arr)\n",
    "                raw_answer.append(int(row[3]))\n",
    "                data.append([resized_arr, row[3]])\n",
    "                #data.append([torch.Tensor(resized_arr), torch.Tensor(int(row[3]))])\n",
    "                # 0 => 있다. \n",
    "                # 1  > 없다. \n",
    "                #data.append(transforms.ToTensor()(resized_arr), row[3]))\n",
    "        except IOError as e:\n",
    "            print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6e4ccdf-a44f-4946-b1fe-436df5c32495",
   "metadata": {},
   "outputs": [],
   "source": [
    "####################################################\n",
    "#       Add Normal and 폐렴 Data for 1:1:1 ratio\n",
    "####################################################\n",
    "normal_path = \"/Users/seungjoobaek/Downloads/chest_xray/chest_xray/train/NORMAL\"\n",
    "normalcnt = 0 \n",
    "for img in os.listdir(normal_path):\n",
    "        # 이미지 전처리 과정이기에 다양한 문제가 발생할 수 있습니다.\n",
    "        # try 문을 활용하여 발생하는 문제를 예외처리 합니다.\n",
    "        try:\n",
    "            # 이미지를 흑백으로 처리하기 위한 과정 입니다.\n",
    "            # cv2.imread() : 이미지 파일을 불러옵니다.\n",
    "            # cv2.IMREAD_GRAYSCALE : 이미지를 흑백으로 처리합니다.\n",
    "            img_arr = cv2.imread(os.path.join(normal_path, img), cv2.IMREAD_GRAYSCALE)\n",
    "            resized_arr = cv2.resize(img_arr, (img_size, img_size))\n",
    "\n",
    "            # 이미지와 이미지의 라벨 정보를 하나로 묶어 data 변수에 할당합니다.\n",
    "            data.append([resized_arr, \"0\"])\n",
    "            normalcnt += 1\n",
    "            if normalcnt == 1137:\n",
    "                break\n",
    "        # 만약 에러가 생기면 어떠한 문제인지를 print() 를 통해 출력합니다.\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "pneu_path = \"/Users/seungjoobaek/Downloads/chest_xray/chest_xray/train/PNEUMONIA\"\n",
    "pneucnt = 0 \n",
    "for img in os.listdir(pneu_path):\n",
    "        # 이미지 전처리 과정이기에 다양한 문제가 발생할 수 있습니다.\n",
    "        # try 문을 활용하여 발생하는 문제를 예외처리 합니다.\n",
    "        try:\n",
    "            # 이미지를 흑백으로 처리하기 위한 과정 입니다.\n",
    "            # cv2.imread() : 이미지 파일을 불러옵니다.\n",
    "            # cv2.IMREAD_GRAYSCALE : 이미지를 흑백으로 처리합니다.\n",
    "            img_arr = cv2.imread(os.path.join(pneu_path, img), cv2.IMREAD_GRAYSCALE)\n",
    "            resized_arr = cv2.resize(img_arr, (img_size, img_size))\n",
    "\n",
    "            # 이미지와 이미지의 라벨 정보를 하나로 묶어 data 변수에 할당합니다.\n",
    "            data.append([resized_arr, \"2\"])\n",
    "            pneucnt += 1\n",
    "            if pneucnt == 1537:\n",
    "                break\n",
    "        # 만약 에러가 생기면 어떠한 문제인지를 print() 를 통해 출력합니다.\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "print(\"updated data size = \", len(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b22792be-b307-4175-9082-01f708676430",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_image example:  [[  0   0   0 ...   0   0   0]\n",
      " [  0   0   0 ...   0   0   0]\n",
      " [  0   0   0 ...   0   0   0]\n",
      " ...\n",
      " [ 65  65  69 ... 177 177 173]\n",
      " [ 69  69  64 ... 179 177 171]\n",
      " [ 69  69  66 ... 182 179 177]]\n",
      "class example:  1\n",
      "\n",
      "Data Size 4611 개\n",
      "================================\n",
      "\n",
      "\n",
      " Data Example : \n",
      " [array([[22, 26, 30, ..., 74,  2,  0],\n",
      "       [37, 44, 72, ..., 79,  1,  0],\n",
      "       [49, 88, 91, ..., 76,  0,  0],\n",
      "       ...,\n",
      "       [ 2,  2, 25, ...,  0,  0,  0],\n",
      "       [ 2,  5, 35, ...,  0,  0,  0],\n",
      "       [ 2,  9, 40, ...,  0,  0,  0]], dtype=uint8), '0']\n",
      "Training Data Size : 3688\n",
      "Validation Data Size : 461\n",
      "Testing Data Size : 462\n"
     ]
    }
   ],
   "source": [
    "####################################################\n",
    "#       Create Train, Valid and Test sets\n",
    "####################################################\n",
    "\n",
    "print('train_image example: ', raw_images[0])\n",
    "print('class example: ', raw_answer[0])\n",
    "size = len(data)\n",
    "print('\\nData Size', size, '개')\n",
    "print('================================')\n",
    "\n",
    "#2.\n",
    "# split train valid from train paths (80,10,10)\n",
    "\n",
    "train_size = int(size * 0.8)\n",
    "validation_size = int(size * 0.1)\n",
    "test_size = size - train_size - validation_size\n",
    "\n",
    "train_dataset, validation_dataset, test_dataset =  random_split(data, [train_size, validation_size, test_size])\n",
    "\n",
    "print('\\n\\n Data Example : \\n', train_dataset[0])\n",
    "print(f\"Training Data Size : {len(train_dataset)}\")\n",
    "print(f\"Validation Data Size : {len(validation_dataset)}\")\n",
    "print(f\"Testing Data Size : {len(test_dataset)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "58da9d79-fa45-477c-a752-48bd1e1366c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#######################################################\n",
    "#               Define Dataset Class\n",
    "#######################################################\n",
    "\n",
    "class CustomImageDataset(Dataset):\n",
    "    def __init__(self, data, transform=False):\n",
    "        self.datas = data\n",
    "        self.transform = transform\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.datas)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.datas[idx]\n",
    "        image = row[0]\n",
    "        label = row[1]\n",
    "        #image = self.images[idx]\n",
    "\n",
    "        #label = class_to_idx[label]\n",
    "        if self.transform is not None:\n",
    "            image = self.transform(to_pil_image(image))\n",
    "        \n",
    "        return image, label\n",
    "    \n",
    "#######################################################\n",
    "#                  Create Dataset\n",
    "#######################################################\n",
    "\n",
    "train_datasets = CustomImageDataset(train_dataset,transform_test)\n",
    "#Augmented Data are added\n",
    "for transform in transform_train:\n",
    "    train_datasets += CustomImageDataset(train_dataset,transform)\n",
    "#others\n",
    "valid_datasets = CustomImageDataset(validation_dataset,transform_test) #test transforms are applied\n",
    "test_datasets = CustomImageDataset(test_dataset,transform_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3ac90c77-dbef-4f3a-a717-44f023f43324",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The shape of tensor for 50th image in train dataset:  torch.Size([1, 128, 128])\n",
      "The label for 50th image in train dataset:  0\n",
      "22128\n",
      "7410 0.3348698481561822 22128\n",
      "1536 0.33311646063760575 4611\n"
     ]
    }
   ],
   "source": [
    "print('The shape of tensor for 50th image in train dataset: ',train_datasets[49][0].shape)\n",
    "print('The label for 50th image in train dataset: ',train_datasets[49][1])\n",
    "print(len(train_datasets))\n",
    "cnt = 0\n",
    "for d in train_datasets :\n",
    "    if d[1] == \"0\" or d[1] == 0: #정상 개수\n",
    "        cnt += 1\n",
    "print(cnt, cnt/len(train_datasets), len(train_datasets))\n",
    "cnt = 0\n",
    "for d in data:\n",
    "    if d[1] == \"0\" or d[1] == 0: #정상 개수\n",
    "        cnt += 1\n",
    "print(cnt, cnt/len(data), len(data)) #동일 비율 20% 확인. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b4ae1702-4d9b-4fe6-9bb1-59c5229a178e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#######################################################\n",
    "#                  Define Dataloaders\n",
    "#######################################################\n",
    "train_dataloader = DataLoader(train_datasets, batch_size=128, shuffle=True)#, drop_last=True)\n",
    "validation_dataloader = DataLoader(valid_datasets, batch_size=128, shuffle=True)#, drop_last=True)\n",
    "test_dataloader = DataLoader(test_datasets, batch_size=128, shuffle=False)#, drop_last=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3e6ef893-a3a8-4388-a98b-91ae9a07d73c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "838cd2ab-30c9-4c78-b06b-e0e83d4a725b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "173\n"
     ]
    }
   ],
   "source": [
    "print(len(train_dataloader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "607bb7d9-f49f-4a6b-9d2e-62d047b51cca",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ResNet(\n",
      "  (conv1): Conv2d(1, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
      "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (relu): ReLU(inplace=True)\n",
      "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
      "  (layer1): Sequential(\n",
      "    (0): BasicBlock(\n",
      "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (layer2): Sequential(\n",
      "    (0): BasicBlock(\n",
      "      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (downsample): Sequential(\n",
      "        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (layer3): Sequential(\n",
      "    (0): BasicBlock(\n",
      "      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (downsample): Sequential(\n",
      "        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (layer4): Sequential(\n",
      "    (0): BasicBlock(\n",
      "      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (downsample): Sequential(\n",
      "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "  (fc): Linear(in_features=512, out_features=1000, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "from torchvision import models\n",
    "import torch\n",
    "# load resnet18 with the pre-trained weights\n",
    "resnet18_pretrained = models.resnet18(pretrained=True)\n",
    "\n",
    "def _get_first_layer(m):\n",
    "    \"Access first layer of a model\"\n",
    "    c,p,n = m,None,None  # child, parent, name\n",
    "    for n in next(m.named_parameters())[0].split('.')[:-1]:\n",
    "        p,c=c,getattr(c,n)\n",
    "    return c,p,n\n",
    "\n",
    "def _load_pretrained_weights(new_layer, previous_layer):\n",
    "    \"Load pretrained weights based on number of input channels\"\n",
    "    n_in = getattr(new_layer, 'in_channels')\n",
    "    if n_in==1:\n",
    "        # we take the sum\n",
    "        new_layer.weight.data = previous_layer.weight.data.sum(dim=1, keepdim=True)\n",
    "    elif n_in==2:\n",
    "        # we take first 2 channels + 50%\n",
    "        new_layer.weight.data = previous_layer.weight.data[:,:2] * 1.5\n",
    "    else:\n",
    "        # keep 3 channels weights and set others to null\n",
    "        new_layer.weight.data[:,:3] = previous_layer.weight.data\n",
    "        new_layer.weight.data[:,3:].zero_()\n",
    "\n",
    "def _update_first_layer(model, n_in, pretrained):\n",
    "    \"Change first layer based on number of input channels\"\n",
    "    if n_in == 3: return\n",
    "    first_layer, parent, name = _get_first_layer(model)\n",
    "    assert isinstance(first_layer, nn.Conv2d), f'Change of input channels only supported with Conv2d, found {first_layer.__class__.__name__}'\n",
    "    assert getattr(first_layer, 'in_channels') == 3, f'Unexpected number of input channels, found {getattr(first_layer, \"in_channels\")} while expecting 3'\n",
    "    params = {attr:getattr(first_layer, attr) for attr in 'out_channels kernel_size stride padding dilation groups padding_mode'.split()}\n",
    "    params['bias'] = getattr(first_layer, 'bias') is not None\n",
    "    params['in_channels'] = n_in\n",
    "    new_layer = nn.Conv2d(**params)\n",
    "    if pretrained:\n",
    "        _load_pretrained_weights(new_layer, first_layer)\n",
    "    setattr(parent, name, new_layer)\n",
    "\n",
    "renet18_pretrained = _update_first_layer(resnet18_pretrained, 1,True)\n",
    "print(resnet18_pretrained)\n",
    "\n",
    "# change the output layer to 2 classes\n",
    "num_classes = 3 # 0 - 정상 , 1 - 기흉 ,  2 - 폐렴\n",
    "num_ftrs = resnet18_pretrained.fc.in_features\n",
    "resnet18_pretrained.fc = nn.Linear(num_ftrs, num_classes)\n",
    "\n",
    "net = resnet18_pretrained.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "343085cc-7491-4beb-bb41-9adaa09f2391",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fun = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(net.parameters(), lr=0.001)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "577b5af7-5f39-4b42-8cc3-2fc92a74a9a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch - 1, Iteration -    30] Loss: 0.895\n",
      "[Epoch - 1, Iteration -    60] Loss: 0.363\n",
      "[Epoch - 1, Iteration -    90] Loss: 0.193\n",
      "[Epoch - 1, Iteration -   120] Loss: 0.107\n",
      "[Epoch - 1, Iteration -   150] Loss: 0.084\n",
      "Check Accuracy of the model on the 461 valid images: 86.11713665943601 % at epoch : 1\n",
      "[Epoch - 1] Loss: 0.581\n",
      "[Epoch - 2, Iteration -    30] Loss: 0.348\n",
      "[Epoch - 2, Iteration -    60] Loss: 0.164\n",
      "[Epoch - 2, Iteration -    90] Loss: 0.125\n",
      "[Epoch - 2, Iteration -   120] Loss: 0.084\n",
      "[Epoch - 2, Iteration -   150] Loss: 0.067\n",
      "Check Accuracy of the model on the 461 valid images: 89.587852494577 % at epoch : 2\n",
      "[Epoch - 2] Loss: 0.339\n",
      "[Epoch - 3, Iteration -    30] Loss: 0.302\n",
      "[Epoch - 3, Iteration -    60] Loss: 0.137\n",
      "[Epoch - 3, Iteration -    90] Loss: 0.091\n",
      "[Epoch - 3, Iteration -   120] Loss: 0.067\n",
      "[Epoch - 3, Iteration -   150] Loss: 0.054\n",
      "Check Accuracy of the model on the 461 valid images: 88.93709327548807 % at epoch : 3\n",
      "[Epoch - 3] Loss: 0.280\n",
      "[Epoch - 4, Iteration -    30] Loss: 0.250\n",
      "[Epoch - 4, Iteration -    60] Loss: 0.129\n",
      "[Epoch - 4, Iteration -    90] Loss: 0.090\n",
      "[Epoch - 4, Iteration -   120] Loss: 0.065\n",
      "[Epoch - 4, Iteration -   150] Loss: 0.048\n",
      "Check Accuracy of the model on the 461 valid images: 90.02169197396964 % at epoch : 4\n",
      "[Epoch - 4] Loss: 0.252\n",
      "[Epoch - 5, Iteration -    30] Loss: 0.251\n",
      "[Epoch - 5, Iteration -    60] Loss: 0.126\n",
      "[Epoch - 5, Iteration -    90] Loss: 0.085\n",
      "[Epoch - 5, Iteration -   120] Loss: 0.063\n",
      "[Epoch - 5, Iteration -   150] Loss: 0.045\n",
      "Check Accuracy of the model on the 461 valid images: 89.3709327548807 % at epoch : 5\n",
      "[Epoch - 5] Loss: 0.244\n",
      "[Epoch - 6, Iteration -    30] Loss: 0.207\n",
      "[Epoch - 6, Iteration -    60] Loss: 0.121\n",
      "[Epoch - 6, Iteration -    90] Loss: 0.066\n",
      "[Epoch - 6, Iteration -   120] Loss: 0.048\n",
      "[Epoch - 6, Iteration -   150] Loss: 0.038\n",
      "Check Accuracy of the model on the 461 valid images: 89.587852494577 % at epoch : 6\n",
      "[Epoch - 6] Loss: 0.204\n",
      "[Epoch - 7, Iteration -    30] Loss: 0.199\n",
      "[Epoch - 7, Iteration -    60] Loss: 0.075\n",
      "[Epoch - 7, Iteration -    90] Loss: 0.063\n",
      "[Epoch - 7, Iteration -   120] Loss: 0.043\n",
      "[Epoch - 7, Iteration -   150] Loss: 0.030\n",
      "Check Accuracy of the model on the 461 valid images: 86.98481561822126 % at epoch : 7\n",
      "[Epoch - 7] Loss: 0.174\n",
      "[Epoch - 8, Iteration -    30] Loss: 0.137\n",
      "[Epoch - 8, Iteration -    60] Loss: 0.070\n",
      "[Epoch - 8, Iteration -    90] Loss: 0.048\n",
      "[Epoch - 8, Iteration -   120] Loss: 0.034\n",
      "[Epoch - 8, Iteration -   150] Loss: 0.024\n",
      "Check Accuracy of the model on the 461 valid images: 89.587852494577 % at epoch : 8\n",
      "[Epoch - 8] Loss: 0.147\n",
      "[Epoch - 9, Iteration -    30] Loss: 0.143\n",
      "[Epoch - 9, Iteration -    60] Loss: 0.053\n",
      "[Epoch - 9, Iteration -    90] Loss: 0.029\n",
      "[Epoch - 9, Iteration -   120] Loss: 0.020\n",
      "[Epoch - 9, Iteration -   150] Loss: 0.013\n",
      "Check Accuracy of the model on the 461 valid images: 89.15401301518438 % at epoch : 9\n",
      "[Epoch - 9] Loss: 0.092\n",
      "[Epoch - 10, Iteration -    30] Loss: 0.056\n",
      "[Epoch - 10, Iteration -    60] Loss: 0.025\n",
      "[Epoch - 10, Iteration -    90] Loss: 0.018\n",
      "[Epoch - 10, Iteration -   120] Loss: 0.011\n",
      "[Epoch - 10, Iteration -   150] Loss: 0.009\n",
      "Check Accuracy of the model on the 461 valid images: 89.3709327548807 % at epoch : 10\n",
      "[Epoch - 10] Loss: 0.048\n",
      "[Epoch - 11, Iteration -    30] Loss: 0.041\n",
      "[Epoch - 11, Iteration -    60] Loss: 0.020\n",
      "[Epoch - 11, Iteration -    90] Loss: 0.010\n",
      "[Epoch - 11, Iteration -   120] Loss: 0.006\n",
      "[Epoch - 11, Iteration -   150] Loss: 0.006\n",
      "Check Accuracy of the model on the 461 valid images: 89.80477223427332 % at epoch : 11\n",
      "[Epoch - 11] Loss: 0.032\n",
      "[Epoch - 12, Iteration -    30] Loss: 0.033\n",
      "[Epoch - 12, Iteration -    60] Loss: 0.009\n",
      "[Epoch - 12, Iteration -    90] Loss: 0.007\n",
      "[Epoch - 12, Iteration -   120] Loss: 0.006\n",
      "[Epoch - 12, Iteration -   150] Loss: 0.004\n",
      "Check Accuracy of the model on the 461 valid images: 89.3709327548807 % at epoch : 12\n",
      "[Epoch - 12] Loss: 0.024\n",
      "Finished Training\n"
     ]
    }
   ],
   "source": [
    "# Train the model\n",
    "epochs = 12\n",
    "  # number of epochs\n",
    "\n",
    "history = []\n",
    "for epoch in range(epochs):\n",
    "\n",
    "    loss_tmp = 0.0\n",
    "    epoch_loss = 0.0 \n",
    "    \n",
    "    for i, datas in enumerate(train_dataloader, start=0):\n",
    "        # Load the data\n",
    "        inputs = datas[0]\n",
    "        labels = [int(a) for a in datas[1]] #넣어진 데이터가 str 이어서 int로 바꿔봄. \n",
    "        labels = torch.tensor(labels) \n",
    "\n",
    "        inputs = inputs.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        # Estimate the output using the network\n",
    "        outputs = net(inputs)\n",
    "\n",
    "        # Calculate the loss between the output of the network and label\n",
    "        loss = loss_fun(outputs, labels)\n",
    "\n",
    "        # Optimize the network \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        loss_tmp += loss.data\n",
    "        epoch_loss += loss.data\n",
    "        if i % 30 == 29:    # Print loss every 5000 mini-batches\n",
    "            print('[Epoch - %d, Iteration - %5d] Loss: %.3f' %(epoch + 1, i + 1, loss_tmp / (i+1)))\n",
    "            loss_tmp = 0.0\n",
    "    \n",
    "    # Update the learning rate according to the learnig rate scheduler\n",
    "    scheduler.step()\n",
    "    \n",
    "    net.eval()  # eval mode (batchnorm uses moving mean/variance instead of mini-batch mean/variance)\n",
    "    with torch.no_grad():\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        for item in validation_dataloader:\n",
    "            inputs = item[0]\n",
    "            labels = [int(a) for a in item[1]]\n",
    "            labels = torch.tensor(labels)\n",
    "    \n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "            outputs = net(inputs)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += len(labels)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "        history.append(100 * correct / total)\n",
    "        print('Check Accuracy of the model on the {} valid images: {} % at epoch : {}'.format(total, 100 * correct / total, epoch+1))\n",
    "        \n",
    "\n",
    "\n",
    "    # Print the epoch loss\n",
    "    print('[Epoch - %d] Loss: %.3f' %(epoch + 1, epoch_loss / (i+1)))\n",
    "\n",
    "print('Finished Training')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20e59458-38f6-4b47-9240-fa4e3ee2908b",
   "metadata": {},
   "outputs": [],
   "source": [
    "net.eval()  # eval mode (batchnorm uses moving mean/variance instead of mini-batch mean/variance)\n",
    "correct = {0 : 0, 1 : 0, 2 : 0}\n",
    "with torch.no_grad():\n",
    "    total_correct = 0\n",
    "    total = 0\n",
    "    for item in test_dataloader:\n",
    "        inputs = item[0]\n",
    "        labels = [int(a) for a in item[1]]\n",
    "        labels = torch.tensor(labels)\n",
    "\n",
    "        inputs = inputs.to(device)\n",
    "        labels = labels.to(device)\n",
    "        outputs = net(inputs)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += len(labels)\n",
    "        \n",
    "        labels = [int(a) for a in labels]\n",
    "        predicted = [int(a) for a in predicted]\n",
    "        for answers in range(len(labels)):\n",
    "            if predicted[answers] == labels[answers] :\n",
    "                correct[labels[answers]] += 1\n",
    "                total_correct += 1\n",
    "        #correct[int(labels)] += (predicted == labels).sum().item()\n",
    "        #correct += (predicted == labels).sum().item()\n",
    "    print('Test Accuracy for Each Class : ')\n",
    "    clslabel = {0 : \"정상\", 1:\"기흉\", 2:\"폐렴\"}\n",
    "    \n",
    "    for cls, cnt in correct.items() : \n",
    "        print('{} 을 맞춘 정확도 : {} %'.format(clslabel[cls], 100 * cnt * 3 / total))\n",
    "    print(' 종합 정확도 {}%'.format(total_correct / total))\n",
    "    #print('Test Accuracy of the model on the {} test images: {} %'.format(total, 100 * correct / (total/3)))\n",
    "print(correct)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "d8357e13-1944-4b51-82e9-b33d7c722dd5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[86.11713665943601, 89.587852494577, 88.93709327548807, 90.02169197396964, 89.3709327548807, 89.587852494577, 86.98481561822126, 89.587852494577, 89.15401301518438, 89.3709327548807, 89.80477223427332, 89.3709327548807]\n"
     ]
    }
   ],
   "source": [
    "print(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a8148d2-2380-487c-ad37-3027325932d1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "virtualenv1.0",
   "language": "python",
   "name": "virtualenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
